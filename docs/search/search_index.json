{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IMLS WIFISESS The WIFISESS data collection pilot has many moving pieces. It involves: Hardware : For the pilot, a participating library must have a Raspberry Pi and one of a (currently limited) number of USB wifi adapters. API Key : Participation requires an API key from api.data.gov , and that key must be approved within the api.data.gov administrative interface. Configuration : The Raspberry Pi must be set up and configured in a particular way. A bash script is used to bootstrap an ansible playbook that downloads the required software, asks the librarian for three pieces of information (API key, FCFS Seq Id, and a tag to identify the Pi), and locks down the device so it cannot be accessed again . Several pieces of custom software were developed to support the configuration of the Pi. Software : On the Pi, we run session-counter , an application we developed to monitor wifi usage and maintain anonymity while reporting back to the data collection backend. It monitors, filters, and then reports data via HTTPS POST to ReVal, a data validator capable of handling a wide variety of inputs. Server Stack : Our server stack involves using the api.data.gov API manager. (This is actually an instance of api-umbrella , an open source tool for managing APIs and keys and throttling requests.) We pass data from api.data.gov to ReVal , a validation library developed by 18F and in use with several federal agencies. If the data validates, we then pass it to Directus , an open source headless CMS; this saved us developing an API for our Postgres instance, as it can automatically inspect a table and present an HTTPS POST API for CRUD operations on those tables. The entire stack is managed as a CloudFoundry buildpack running on cloud.gov . Everything used in the pilot is free and open source software. This site documents each of these pieces: how to maintain, build, test, and deploy the components involved.","title":"Home"},{"location":"#imls-wifisess","text":"The WIFISESS data collection pilot has many moving pieces. It involves: Hardware : For the pilot, a participating library must have a Raspberry Pi and one of a (currently limited) number of USB wifi adapters. API Key : Participation requires an API key from api.data.gov , and that key must be approved within the api.data.gov administrative interface. Configuration : The Raspberry Pi must be set up and configured in a particular way. A bash script is used to bootstrap an ansible playbook that downloads the required software, asks the librarian for three pieces of information (API key, FCFS Seq Id, and a tag to identify the Pi), and locks down the device so it cannot be accessed again . Several pieces of custom software were developed to support the configuration of the Pi. Software : On the Pi, we run session-counter , an application we developed to monitor wifi usage and maintain anonymity while reporting back to the data collection backend. It monitors, filters, and then reports data via HTTPS POST to ReVal, a data validator capable of handling a wide variety of inputs. Server Stack : Our server stack involves using the api.data.gov API manager. (This is actually an instance of api-umbrella , an open source tool for managing APIs and keys and throttling requests.) We pass data from api.data.gov to ReVal , a validation library developed by 18F and in use with several federal agencies. If the data validates, we then pass it to Directus , an open source headless CMS; this saved us developing an API for our Postgres instance, as it can automatically inspect a table and present an HTTPS POST API for CRUD operations on those tables. The entire stack is managed as a CloudFoundry buildpack running on cloud.gov . Everything used in the pilot is free and open source software. This site documents each of these pieces: how to maintain, build, test, and deploy the components involved.","title":"IMLS WIFISESS"},{"location":"repositories/","text":"Project repositories This project has three major pieces: Sensors . The wifi session \"sensors\" are Raspberry Pis running a locked-down Debian 9 and custom software from 18F. These are complex in their own right, because they need to be set up by non-expert users and operate safely in a potentially hostile environment. Backend . The data must be collected, stored, and backed up. This is the backend. For the pilot, it is a software stack running under provisional ATO on cloud.gov . Presentation . What does the data look like? While IMLS has a full Tableau stack, the rest of the world does not. We will mock up some minimal examples in pure JS and (possibly) Jupyter Notebooks (Google Collab) for end-users to explore/use as starting points for checking their own library's data. (Not yet started.) Documentation . The documentation for the tools is interspersed throughout the repositories, but the bulk of the docs are centralized in the Federalist site, which was stood up to support librarians taking part in the pilot. Sensor repositories These repositories are directly related to the Raspberry Pi \"sensors.\" imls-client-pi-playbook The imls-client-pi-playbook repository bootstraps the RPi from a default state to being fully operational as a participant in the data collection network. To use the playbook, a librarian first installs the Raspberry Pi OS. This is, for all intents and purposes, Debian 9. They then run our bootstrap: bash <(curl -s https://raw.githubusercontent.com/jadudm/imls-client-pi-playbook/main/bootstrap.sh) This runs a go application for reading in (and verifying) their setup parameters, updates the RPi to the most recent version of Ansible, and then pulls the playbook itself ( git clone ) and runs it. As part of this, the playbook installs the data collection software, locks down the system (amongst other things, running DevSec hardening profiles, disabling all external network access, and disabling interactive login users), and sets itself up to pull and rerun the playbook once per day. (This way, we can update the playbook and update the devices if needed.) That said, once the devices are set up, it is not possible for us, or a librarian, to get back in and make changes. The playbook is a \"one way trip to lockdown.\" input-initial-configuration One of the first programs that is executed is input-initial-configuration . This Golang program does three things: It reads in the api.data.gov key as a series of two-word phrases. Each two-word phrase is mapped to three ASCII characters. We deemed this less error prone than having the librarian type a 40-character API key manually, because we cannot check the key. However, we can verify that each two-word phrase is a valid phrase, and as a result, there is more integrity in the key entry process at setup time. It reads in the FCFS Seq Id for where the sensor will be installed; it checks that the state is valid, and that the right number of digits are provided. For the future, it might be good to have a complete list of valid Seq Ids, but the danger would be if that falls out of date... It asks for a \"hardware tag,\" which is a freeform (255-character) field. The librarian might say the device is \"Device001,\" or \"reference desk.\" Either way, it is intended as a local identifier. Once this data is read in, the API key is encrypted and written to disk for use by other tools in the stack at a later point. input-initial-configuration does not communicate with the outside world. We have debated whether or not it should record an event after it is done, so that we know when a librarian has attempted the setup process. find-ralink As part of setup, find-ralink is used to discover valid wifi adapters for sensing. This custom Golang program can both determine whether or not a valid USB wifi adapter is present ( find-ralink --exists ) as well as read specific properties about the hardware ( find-ralink --extract mac ). This utility is used as part of the playbook for device setup and configuration at the OS level. find-ralink does not communicate with the outside world. go-session-counter session-counter is a Golang application that runs \"forever.\" It spends 45 seconds monitoring for devices, anonymizes things, and then sends a report once per minute to the backend. It relies on the encrypted API key laid down by input-initial-configuration . If session-counter encounters too many HTTPS errors in a given timeframe, we quit. This way, the systemd unit installed by the playbook can restart session-counter , and hopefully data collection can resume uninterrupted. (There are many, many reasons data collection could be interrupted, and may require more thought if scaling to additional participants is being considered.) Backend The backend is a cloud.gov buildpack . This stands up our database (Postgres), the API provider (Directus), and the validation framework (ReVaL). cloud.gov is otherwise known as Cloud Foundry, an open source hosting framework inspired by Hiroku (and similar). We have not, at this time, automated the configuration and management of api.data.gov. However, there are not enough participants in the pilot to have made this level of automation a priority. For scaling, thinking about how to make it easy to add/remove keys from the set \"allowed to store data\" is something that we would need to design/develop. Presentation We have not yet developed any scripts to present the data being collected. Accessing data can either be public (a download), a public API (open for read via api.data.gov), or by permission. If by API, it is possible to use a single GET command to extract/query data; Directus provides a rich API interface, and for the pilot makes it easy to extract some or all of the data matching simple query parameters. Documentation The documentation for the project lives (in part) in the component repositories, and in part in two additional website-based repositories. imls-handoff is a mkdocs site. It can be used directly, and provides markdown-based documentation of the stack. This page is part of imls-handoff . 10x-shared-components-phase-3 is a Federalist site. Federalist is a platform developed/provided by 18F/TTS/GSA intended to provide a low bar for secure, static-site hosting. Underneath, it is Jekyll, a static site generator. This portion of the documentation was developed for informing the IMLS/library community about the work, as well as documenting the setup process of the RPis for participating libraries. It embeds the imls-handoff docs as a submodule.","title":"Repositories"},{"location":"repositories/#project-repositories","text":"This project has three major pieces: Sensors . The wifi session \"sensors\" are Raspberry Pis running a locked-down Debian 9 and custom software from 18F. These are complex in their own right, because they need to be set up by non-expert users and operate safely in a potentially hostile environment. Backend . The data must be collected, stored, and backed up. This is the backend. For the pilot, it is a software stack running under provisional ATO on cloud.gov . Presentation . What does the data look like? While IMLS has a full Tableau stack, the rest of the world does not. We will mock up some minimal examples in pure JS and (possibly) Jupyter Notebooks (Google Collab) for end-users to explore/use as starting points for checking their own library's data. (Not yet started.) Documentation . The documentation for the tools is interspersed throughout the repositories, but the bulk of the docs are centralized in the Federalist site, which was stood up to support librarians taking part in the pilot.","title":"Project repositories"},{"location":"repositories/#sensor-repositories","text":"These repositories are directly related to the Raspberry Pi \"sensors.\"","title":"Sensor repositories"},{"location":"repositories/#imls-client-pi-playbook","text":"The imls-client-pi-playbook repository bootstraps the RPi from a default state to being fully operational as a participant in the data collection network. To use the playbook, a librarian first installs the Raspberry Pi OS. This is, for all intents and purposes, Debian 9. They then run our bootstrap: bash <(curl -s https://raw.githubusercontent.com/jadudm/imls-client-pi-playbook/main/bootstrap.sh) This runs a go application for reading in (and verifying) their setup parameters, updates the RPi to the most recent version of Ansible, and then pulls the playbook itself ( git clone ) and runs it. As part of this, the playbook installs the data collection software, locks down the system (amongst other things, running DevSec hardening profiles, disabling all external network access, and disabling interactive login users), and sets itself up to pull and rerun the playbook once per day. (This way, we can update the playbook and update the devices if needed.) That said, once the devices are set up, it is not possible for us, or a librarian, to get back in and make changes. The playbook is a \"one way trip to lockdown.\"","title":"imls-client-pi-playbook"},{"location":"repositories/#input-initial-configuration","text":"One of the first programs that is executed is input-initial-configuration . This Golang program does three things: It reads in the api.data.gov key as a series of two-word phrases. Each two-word phrase is mapped to three ASCII characters. We deemed this less error prone than having the librarian type a 40-character API key manually, because we cannot check the key. However, we can verify that each two-word phrase is a valid phrase, and as a result, there is more integrity in the key entry process at setup time. It reads in the FCFS Seq Id for where the sensor will be installed; it checks that the state is valid, and that the right number of digits are provided. For the future, it might be good to have a complete list of valid Seq Ids, but the danger would be if that falls out of date... It asks for a \"hardware tag,\" which is a freeform (255-character) field. The librarian might say the device is \"Device001,\" or \"reference desk.\" Either way, it is intended as a local identifier. Once this data is read in, the API key is encrypted and written to disk for use by other tools in the stack at a later point. input-initial-configuration does not communicate with the outside world. We have debated whether or not it should record an event after it is done, so that we know when a librarian has attempted the setup process.","title":"input-initial-configuration"},{"location":"repositories/#find-ralink","text":"As part of setup, find-ralink is used to discover valid wifi adapters for sensing. This custom Golang program can both determine whether or not a valid USB wifi adapter is present ( find-ralink --exists ) as well as read specific properties about the hardware ( find-ralink --extract mac ). This utility is used as part of the playbook for device setup and configuration at the OS level. find-ralink does not communicate with the outside world.","title":"find-ralink"},{"location":"repositories/#go-session-counter","text":"session-counter is a Golang application that runs \"forever.\" It spends 45 seconds monitoring for devices, anonymizes things, and then sends a report once per minute to the backend. It relies on the encrypted API key laid down by input-initial-configuration . If session-counter encounters too many HTTPS errors in a given timeframe, we quit. This way, the systemd unit installed by the playbook can restart session-counter , and hopefully data collection can resume uninterrupted. (There are many, many reasons data collection could be interrupted, and may require more thought if scaling to additional participants is being considered.)","title":"go-session-counter"},{"location":"repositories/#backend","text":"The backend is a cloud.gov buildpack . This stands up our database (Postgres), the API provider (Directus), and the validation framework (ReVaL). cloud.gov is otherwise known as Cloud Foundry, an open source hosting framework inspired by Hiroku (and similar). We have not, at this time, automated the configuration and management of api.data.gov. However, there are not enough participants in the pilot to have made this level of automation a priority. For scaling, thinking about how to make it easy to add/remove keys from the set \"allowed to store data\" is something that we would need to design/develop.","title":"Backend"},{"location":"repositories/#presentation","text":"We have not yet developed any scripts to present the data being collected. Accessing data can either be public (a download), a public API (open for read via api.data.gov), or by permission. If by API, it is possible to use a single GET command to extract/query data; Directus provides a rich API interface, and for the pilot makes it easy to extract some or all of the data matching simple query parameters.","title":"Presentation"},{"location":"repositories/#documentation","text":"The documentation for the project lives (in part) in the component repositories, and in part in two additional website-based repositories. imls-handoff is a mkdocs site. It can be used directly, and provides markdown-based documentation of the stack. This page is part of imls-handoff . 10x-shared-components-phase-3 is a Federalist site. Federalist is a platform developed/provided by 18F/TTS/GSA intended to provide a low bar for secure, static-site hosting. Underneath, it is Jekyll, a static site generator. This portion of the documentation was developed for informing the IMLS/library community about the work, as well as documenting the setup process of the RPis for participating libraries. It embeds the imls-handoff docs as a submodule.","title":"Documentation"},{"location":"hardware/pi/","text":"Raspberry Pi For the pilot, we settled on the Raspberry Pi as an implementation platform. It is low-cost open hardware, and is readily available both from brick-and-mortar retailers as well as many web outlets. For the pilot, we used the Model 4b . This makes it a quad-core, 1.5GHz ARM-based Debian device with 4GB of RAM. We have also tested on a 3B+ with 1GB of RAM. It is our belief that a low-spec model (3B or 4B with 1GB or 2GB of RAM) and a 16GB microSD card or larger will more-than-adequately work as a platform for this pilot. For the Future It is possible for an old Intel-based laptop or computer to serve as the host for this project. We believe the number of changes required (to adapt to the new operating system) far outweigh the low cost and ready availability of the Raspberry Pi.","title":"Pi"},{"location":"hardware/pi/#raspberry-pi","text":"For the pilot, we settled on the Raspberry Pi as an implementation platform. It is low-cost open hardware, and is readily available both from brick-and-mortar retailers as well as many web outlets. For the pilot, we used the Model 4b . This makes it a quad-core, 1.5GHz ARM-based Debian device with 4GB of RAM. We have also tested on a 3B+ with 1GB of RAM. It is our belief that a low-spec model (3B or 4B with 1GB or 2GB of RAM) and a 16GB microSD card or larger will more-than-adequately work as a platform for this pilot.","title":"Raspberry Pi"},{"location":"hardware/pi/#for-the-future","text":"It is possible for an old Intel-based laptop or computer to serve as the host for this project. We believe the number of changes required (to adapt to the new operating system) far outweigh the low cost and ready availability of the Raspberry Pi.","title":"For the Future"},{"location":"hardware/wifi/","text":"WiFi The wifi chipset built into the Raspberry Pi is not capable of listening (generally) to ambient wifi due to BIOS limitations. For that reason, we chose to use an external wifi device. The Ralink chipset is widely supported under Linux, and low cost devices can easily be found from many retailiers. We used the PAU06 and PAU09 devices (2.4GHz and 5GHz, respectively) for testing in the pilot. For the future We developed a small application ( find-ralink ) that searches out and provides configuration about these devices when they are plugged into the RPi. It is a \"hardware search tool\" of sorts, and can be extended to support/find additional chipsets and hardware in the future. As additional devices are confirmed to work, the search list can be extended, and the utility of the tool improved.","title":"Wifi Adapters"},{"location":"hardware/wifi/#wifi","text":"The wifi chipset built into the Raspberry Pi is not capable of listening (generally) to ambient wifi due to BIOS limitations. For that reason, we chose to use an external wifi device. The Ralink chipset is widely supported under Linux, and low cost devices can easily be found from many retailiers. We used the PAU06 and PAU09 devices (2.4GHz and 5GHz, respectively) for testing in the pilot.","title":"WiFi"},{"location":"hardware/wifi/#for-the-future","text":"We developed a small application ( find-ralink ) that searches out and provides configuration about these devices when they are plugged into the RPi. It is a \"hardware search tool\" of sorts, and can be extended to support/find additional chipsets and hardware in the future. As additional devices are confirmed to work, the search list can be extended, and the utility of the tool improved.","title":"For the future"},{"location":"queries/obtainkey/","text":"Querying data There are two interfaces for querying data: GraphQL and a RESTful interface. The API is being provided by directus.io ; therefore, the API documentation for Directus applies here, and reviewing it is worthwhile. Here, we will provide a high-level summary of how to extract data from the IMLS WIFISESS pilot. Obtain an API key First, you will need an API key. Go to api.data.gov's signup page and sign up for a key. Shortly after signup, you should receive an API key via email. All requests are routed through api.data.gov. It provides us with a layer of security, key management, rate limiting, and other services we felt no need to develop (since they already existed). We recommend it highly.","title":"Obtain a key"},{"location":"queries/obtainkey/#querying-data","text":"There are two interfaces for querying data: GraphQL and a RESTful interface. The API is being provided by directus.io ; therefore, the API documentation for Directus applies here, and reviewing it is worthwhile. Here, we will provide a high-level summary of how to extract data from the IMLS WIFISESS pilot.","title":"Querying data"},{"location":"queries/obtainkey/#obtain-an-api-key","text":"First, you will need an API key. Go to api.data.gov's signup page and sign up for a key. Shortly after signup, you should receive an API key via email. All requests are routed through api.data.gov. It provides us with a layer of security, key management, rate limiting, and other services we felt no need to develop (since they already existed). We recommend it highly.","title":"Obtain an API key"},{"location":"queries/schemas/","text":"We don't claim the v1 tables are... normalized , or anything like that. But, they're going to work for the pilot. events_v1 We have an \"events\" table that we log a few different things to. Primarily, we capture startup events (so we know when the Pi goes live), and logging_devices , which tells us that we are starting a minute-long logging and reporting cycle. The schema for this table is: CREATE TABLE public.events_v1 ( id serial PRIMARY KEY, pi_serial character varying(16), fcfs_seq_id character varying(16), device_tag character varying(32), session_id character varying(255), \"localtime\" timestamp with time zone, servertime timestamp with time zone DEFAULT current_timestamp, tag character varying(255), info text ); When writing queries (via GQL or REST) these column names are what you'll want to use in your queries. We envision this table could be replaced in the future by a proper logging framework . wifi_v1 The \"wifi\" table captures device data every minute. The schema is: CREATE TABLE public.wifi_v1 ( id serial PRIMARY KEY, event_id integer, pi_serial character varying(16), fcfs_seq_id character varying(16), device_tag character varying(32), \"localtime\" timestamp with time zone, servertime timestamp with time zone DEFAULT current_timestamp, -- session_id: the unique identifier for session, randomly generated when session-counter starts session_id character varying(255), manufacturer_index integer, patron_index integer ); (The redundancy in these tables is impressive.) Every time the Pi powers up, it gets a new session_id . This means that pulling all the wifi events from a single session_id will give you everything from a single power cycle. (Note that while we do reboot every night, it is possible the Pi could restart for other reasons at other times. These are sensors in the wild, and there are many, many reasons a device might reboot.) We log data every minute. Each device seen gets a row in the table. (Disk is cheap?) To pull all the devices seen in a given minute, use the event_id . If 100 devices were seen in a given minute, then we will have 100 rows in the table with the same event_id . Making sense of the data Our next task is to develop some analyses that do low- and high-pass filtering on the data. That is, if we see a device every minute of every hour of the day, it is probably not a user of the wifi. Likewise, devices we see for 5-or-fewer-minutes could be considered transitory, and should be filtered out as well. This is why we are doing the pilot: until now, the behavior of wifi users has been conjectural. We will now have 17 sensors reporting data that we can look at, compare, and attempt to make sense of.","title":"The Schemas"},{"location":"queries/schemas/#events_v1","text":"We have an \"events\" table that we log a few different things to. Primarily, we capture startup events (so we know when the Pi goes live), and logging_devices , which tells us that we are starting a minute-long logging and reporting cycle. The schema for this table is: CREATE TABLE public.events_v1 ( id serial PRIMARY KEY, pi_serial character varying(16), fcfs_seq_id character varying(16), device_tag character varying(32), session_id character varying(255), \"localtime\" timestamp with time zone, servertime timestamp with time zone DEFAULT current_timestamp, tag character varying(255), info text ); When writing queries (via GQL or REST) these column names are what you'll want to use in your queries. We envision this table could be replaced in the future by a proper logging framework .","title":"events_v1"},{"location":"queries/schemas/#wifi_v1","text":"The \"wifi\" table captures device data every minute. The schema is: CREATE TABLE public.wifi_v1 ( id serial PRIMARY KEY, event_id integer, pi_serial character varying(16), fcfs_seq_id character varying(16), device_tag character varying(32), \"localtime\" timestamp with time zone, servertime timestamp with time zone DEFAULT current_timestamp, -- session_id: the unique identifier for session, randomly generated when session-counter starts session_id character varying(255), manufacturer_index integer, patron_index integer ); (The redundancy in these tables is impressive.) Every time the Pi powers up, it gets a new session_id . This means that pulling all the wifi events from a single session_id will give you everything from a single power cycle. (Note that while we do reboot every night, it is possible the Pi could restart for other reasons at other times. These are sensors in the wild, and there are many, many reasons a device might reboot.) We log data every minute. Each device seen gets a row in the table. (Disk is cheap?) To pull all the devices seen in a given minute, use the event_id . If 100 devices were seen in a given minute, then we will have 100 rows in the table with the same event_id .","title":"wifi_v1"},{"location":"queries/schemas/#making-sense-of-the-data","text":"Our next task is to develop some analyses that do low- and high-pass filtering on the data. That is, if we see a device every minute of every hour of the day, it is probably not a user of the wifi. Likewise, devices we see for 5-or-fewer-minutes could be considered transitory, and should be filtered out as well. This is why we are doing the pilot: until now, the behavior of wifi users has been conjectural. We will now have 17 sensors reporting data that we can look at, compare, and attempt to make sense of.","title":"Making sense of the data"},{"location":"queries/summary/","text":"That's It In summary: Obtain an API key from api.data.gov. Formulate queries using GQL or RESTful GET s. Process the resulting JSON.","title":"Summary"},{"location":"queries/summary/#thats-it","text":"In summary: Obtain an API key from api.data.gov. Formulate queries using GQL or RESTful GET s. Process the resulting JSON.","title":"That's It"},{"location":"queries/testkey/","text":"Test your key We have a simple web page for you to test your key . Enter your key Look for an FCFS_Seq_Id that is part of the pilot. We have two test IDs: CA0001-001 and ME0064-001 . Look for a device tag in those FCFS Ids. In this case, each of the test devices has a different tag: California is in the depths of Z'ha'dum and Maine is in the basement . If your key works, you'll get something back. (It might be that you get nothing back; however, if you see a graph come up---even if it is empty---it means that the key worked.) If the key does not work, you will get an error, and the page will show it. (We don't know how robust this test is. It was written quickly, and last. We believe it is good enough for the librarians who are taking part in the pilot to find out if their devices are running after they set them up. That's the intent/limit of what we intended this page for. If you encounter problems with the test page, please let us know, submit patches, etc.)","title":"Test your key"},{"location":"queries/testkey/#test-your-key","text":"We have a simple web page for you to test your key . Enter your key Look for an FCFS_Seq_Id that is part of the pilot. We have two test IDs: CA0001-001 and ME0064-001 . Look for a device tag in those FCFS Ids. In this case, each of the test devices has a different tag: California is in the depths of Z'ha'dum and Maine is in the basement . If your key works, you'll get something back. (It might be that you get nothing back; however, if you see a graph come up---even if it is empty---it means that the key worked.) If the key does not work, you will get an error, and the page will show it. (We don't know how robust this test is. It was written quickly, and last. We believe it is good enough for the librarians who are taking part in the pilot to find out if their devices are running after they set them up. That's the intent/limit of what we intended this page for. If you encounter problems with the test page, please let us know, submit patches, etc.)","title":"Test your key"},{"location":"queries/usinggql/","text":"Using graphql (GQL) On your own service backend, querying the data via graphql is straight-forward (if you understand GQL). We do not have expertise with GQL, but it is essentially an endpoint that lets you submit queries as JSON documents. On our team, we have used GraphiQL as an open source application for testing queries before embedding them into applications. Here is an example query: { items { wifi_v1(filter: {fcfs_seq_id:{_eq:\"<THESEQID>\"}, device_tag: {_eq: \"<THETAG>\"}}) { device_tag session_id event_id manufacturer_index patron_index } } } This query will, when executed, return an array of objects (or an empty array). Each object will have five fields as specified in the query ( device_tag , session_id , etc.). It will only return objects that meet the constraints; in this case, where the fcfs_seq_id is equal to the ID passed in, and the device_tag is similarly a match. (This is the WHERE clause of the query). There are additional constraints/filters that can be placed on a query, and we would refer you to the Directus documentation for more info. Once you have tested the query ( GraphiQL ), you can embed it in your application as an HTTPS POST. Your target endpoint is (during the pilot) https://api.data.gov/TEST/10x-imls/v1/graphql/ . Here is an example from the client side in Javascript. (Do not embed your key in a Javascript application. You know this.) // Because of CORs, we need to pass the API key as a URL // parameter. The COR constraints can be lifted, but // for now we have chosen to leave them in place. function gqlUrl (key) { return `https://api.data.gov/TEST/10x-imls/v1/graphql/?api_key=${key}`; } // The query options need to wrap the query in an object. // That is, the JSON submitted to the server must have // the form: // { query: ... } // where the `...` is a valid query as assembled in // GraphiQL or similar. function gqlOptions(query) { const options = { method: \"POST\", headers: { \"Content-Type\": \"application/json\", }, body: JSON.stringify({ query: query }) }; return options; } // Here, we have parameterized the wifiQuery string // using several variables that were set earlier in the // code. That code is not shown in this example; it // pulls the values from some text fields on a webpage. // It would be safe to embed query parameters in a page, // but not the key itself! var wifiQuery = ` { items { wifi_v1(limit: ${SEARCH_LIMIT}, filter: { fcfs_seq_id: {_eq: \"${fcfs_seq_id}\"}, device_tag: {_eq: \"${device_tag}\"} ) { device_tag session_id event_id manufacturer_index patron_index } } }`; // Now we do the fetch, handling success and failure // as appropriate for the application at hand. await fetch(gqlUrl(api_key), gqlOptions(eventQuery)) .then(res => res.json()) .then(eventsResult) .catch(eventFailHandler); In Python, Go, or any other language, the steps are the same: Formulate and test a valid GQL query. Wrap the query in a JSON object of the form {query: ...} POST that query to https://api.data.gov/TEST/10x-imls/v1/graphql/ The trailing slash on that URL matters . If the query is successful, you will get back something that looks like: { \"data\": { \"items\": \"events_v1\": [ ... objects ... ] } } where events_v1 will be whichever table you have queried; for the pilot, events_v1 and wifi_v1 are the only tables available. The objects returned will be JSON objects containing the fields you specified. If you only indicated you wanted device_tag , then each object will contain one field only: device_tag . NOTE : We have left the pi_serial field out of the results that come back from all queries; why? An over-abundance of caution. It is not PII (because we own the RPis in question, and it does not indicate anything about an individual ), but we left it out of queries just the same. This applies to GQL and RESTful queries alike.","title":"Using GraphQL"},{"location":"queries/usinggql/#using-graphql-gql","text":"On your own service backend, querying the data via graphql is straight-forward (if you understand GQL). We do not have expertise with GQL, but it is essentially an endpoint that lets you submit queries as JSON documents. On our team, we have used GraphiQL as an open source application for testing queries before embedding them into applications. Here is an example query: { items { wifi_v1(filter: {fcfs_seq_id:{_eq:\"<THESEQID>\"}, device_tag: {_eq: \"<THETAG>\"}}) { device_tag session_id event_id manufacturer_index patron_index } } } This query will, when executed, return an array of objects (or an empty array). Each object will have five fields as specified in the query ( device_tag , session_id , etc.). It will only return objects that meet the constraints; in this case, where the fcfs_seq_id is equal to the ID passed in, and the device_tag is similarly a match. (This is the WHERE clause of the query). There are additional constraints/filters that can be placed on a query, and we would refer you to the Directus documentation for more info. Once you have tested the query ( GraphiQL ), you can embed it in your application as an HTTPS POST. Your target endpoint is (during the pilot) https://api.data.gov/TEST/10x-imls/v1/graphql/ . Here is an example from the client side in Javascript. (Do not embed your key in a Javascript application. You know this.) // Because of CORs, we need to pass the API key as a URL // parameter. The COR constraints can be lifted, but // for now we have chosen to leave them in place. function gqlUrl (key) { return `https://api.data.gov/TEST/10x-imls/v1/graphql/?api_key=${key}`; } // The query options need to wrap the query in an object. // That is, the JSON submitted to the server must have // the form: // { query: ... } // where the `...` is a valid query as assembled in // GraphiQL or similar. function gqlOptions(query) { const options = { method: \"POST\", headers: { \"Content-Type\": \"application/json\", }, body: JSON.stringify({ query: query }) }; return options; } // Here, we have parameterized the wifiQuery string // using several variables that were set earlier in the // code. That code is not shown in this example; it // pulls the values from some text fields on a webpage. // It would be safe to embed query parameters in a page, // but not the key itself! var wifiQuery = ` { items { wifi_v1(limit: ${SEARCH_LIMIT}, filter: { fcfs_seq_id: {_eq: \"${fcfs_seq_id}\"}, device_tag: {_eq: \"${device_tag}\"} ) { device_tag session_id event_id manufacturer_index patron_index } } }`; // Now we do the fetch, handling success and failure // as appropriate for the application at hand. await fetch(gqlUrl(api_key), gqlOptions(eventQuery)) .then(res => res.json()) .then(eventsResult) .catch(eventFailHandler); In Python, Go, or any other language, the steps are the same: Formulate and test a valid GQL query. Wrap the query in a JSON object of the form {query: ...} POST that query to https://api.data.gov/TEST/10x-imls/v1/graphql/ The trailing slash on that URL matters . If the query is successful, you will get back something that looks like: { \"data\": { \"items\": \"events_v1\": [ ... objects ... ] } } where events_v1 will be whichever table you have queried; for the pilot, events_v1 and wifi_v1 are the only tables available. The objects returned will be JSON objects containing the fields you specified. If you only indicated you wanted device_tag , then each object will contain one field only: device_tag . NOTE : We have left the pi_serial field out of the results that come back from all queries; why? An over-abundance of caution. It is not PII (because we own the RPis in question, and it does not indicate anything about an individual ), but we left it out of queries just the same. This applies to GQL and RESTful queries alike.","title":"Using graphql (GQL)"},{"location":"queries/viarest/","text":"Using RESTful queries Directus also has a REST api for queries. We have provided two endpoints for read-only queries: https://api.data.gov/TEST/10x-imls/v1/search/events/ https://api.data.gov/TEST/10x-imls/v1/search/wifi/ These route to the appropriate endpoints in Directus, with permissions that limit the queries to being read-only. Like the GQL queries, a query to the backend is GET able via HTTPS. The key is passed in the X-Api-Key header (as per the api.data.gov documentation), and the query itself is formed as part of the URL. When we route the above URLs to Directus, we are rewriting /events to /items/events_v1/ , and /wifi/ to /items/wifi_v1 . This means you cannot use the Directus API in a general sense; you can only use it to extract items from the data we've collected. (More of the Directus API can be exposed easily, but we have chosen to open the door just wide enough for the moment.) For example, to extract objects from the events_v1 table, we would GET our query to https://api.data.gov/TEST/10x-imls/v1/search/events/ and the last 100 events from that table would be returned. If we want to restrict the query in some way, we can use global query parameters from Directus. https://api.data.gov/TEST/10x-imls/v1/search/events/?fields=session_id will return the last 100 events, but only the session_id field. It is also possible to filter events : https://api.data.gov/TEST/10x-imls/v1/search/events/?filter[fcfs_seq_id][_eq]=\"ME0064-001 would return events from one of the two dev/test RPis. Here is a curl command that uses this interface: FILTER1=\"filter\\[tag\\]\\[_eq\\]=startup\" FILTER2=\"filter\\[fcfs_seq_id\\]\\[_eq\\]=ME0064-001\" curl \\ -X GET \\ -H \"X-Api-Key: $APIDATAGOVKEY\" \\ \"https://api.data.gov/TEST/10x-imls/v1/search/events/?$FILTER1&$FILTER2\" That query should, if the environment variable APIDATAGOVKEY is set, return the last 100 events for the matching fcfs_seq_id and with the tag startup . (The tag is an event tag , and tells us what kind of event was being logged.) As with GQL, the results come back as an array of JSON objects. For testing, we pipe the results through jq for prettying. And, if you grab some additional tools, you can even convert the JSON into CSV. Assuming you go get the program json2csv and put the above code into a file called q.curl : ./q.curl | jq -c -r \".data[]\" | ~/go/bin/json2csv -k servertime,session_id,tag you can get a CSV version of the same data. (Of course, in an application, you could either use a library to do this, or you would walk the resulting JSON objects and convert them to CSV yourself -- if you needed the data in a tabular form. Libraries like pandas have tooling built-in to convert arrays of JSON objects to dataframes, for example.) Note, when working with curl on the command line, that significant escaping of brackets becomes necessary. In other languages, \"your mileage may vary.\"","title":"Using REST"},{"location":"queries/viarest/#using-restful-queries","text":"Directus also has a REST api for queries. We have provided two endpoints for read-only queries: https://api.data.gov/TEST/10x-imls/v1/search/events/ https://api.data.gov/TEST/10x-imls/v1/search/wifi/ These route to the appropriate endpoints in Directus, with permissions that limit the queries to being read-only. Like the GQL queries, a query to the backend is GET able via HTTPS. The key is passed in the X-Api-Key header (as per the api.data.gov documentation), and the query itself is formed as part of the URL. When we route the above URLs to Directus, we are rewriting /events to /items/events_v1/ , and /wifi/ to /items/wifi_v1 . This means you cannot use the Directus API in a general sense; you can only use it to extract items from the data we've collected. (More of the Directus API can be exposed easily, but we have chosen to open the door just wide enough for the moment.) For example, to extract objects from the events_v1 table, we would GET our query to https://api.data.gov/TEST/10x-imls/v1/search/events/ and the last 100 events from that table would be returned. If we want to restrict the query in some way, we can use global query parameters from Directus. https://api.data.gov/TEST/10x-imls/v1/search/events/?fields=session_id will return the last 100 events, but only the session_id field. It is also possible to filter events : https://api.data.gov/TEST/10x-imls/v1/search/events/?filter[fcfs_seq_id][_eq]=\"ME0064-001 would return events from one of the two dev/test RPis. Here is a curl command that uses this interface: FILTER1=\"filter\\[tag\\]\\[_eq\\]=startup\" FILTER2=\"filter\\[fcfs_seq_id\\]\\[_eq\\]=ME0064-001\" curl \\ -X GET \\ -H \"X-Api-Key: $APIDATAGOVKEY\" \\ \"https://api.data.gov/TEST/10x-imls/v1/search/events/?$FILTER1&$FILTER2\" That query should, if the environment variable APIDATAGOVKEY is set, return the last 100 events for the matching fcfs_seq_id and with the tag startup . (The tag is an event tag , and tells us what kind of event was being logged.) As with GQL, the results come back as an array of JSON objects. For testing, we pipe the results through jq for prettying. And, if you grab some additional tools, you can even convert the JSON into CSV. Assuming you go get the program json2csv and put the above code into a file called q.curl : ./q.curl | jq -c -r \".data[]\" | ~/go/bin/json2csv -k servertime,session_id,tag you can get a CSV version of the same data. (Of course, in an application, you could either use a library to do this, or you would walk the resulting JSON objects and convert them to CSV yourself -- if you needed the data in a tabular form. Libraries like pandas have tooling built-in to convert arrays of JSON objects to dataframes, for example.) Note, when working with curl on the command line, that significant escaping of brackets becomes necessary. In other languages, \"your mileage may vary.\"","title":"Using RESTful queries"},{"location":"setup/bootstrap/","text":"Bootstrap To set up a Raspberry Pi, a librarian first needs to install the Raspbian OS onto a microSD card, and then they need to run our bootstrap script. Installing the OS x To setup the Raspberry Pi, we first ask the user to download the Raspberry Pi Imager . This is a free and open tool for Mac, Windows, and Linux that aids in the creation of microSD cards for the Pi. The imaging tool allows the user to select their operating system (there are multiple options), and then install it on a microSD card plugged into their computer. It is free, open, reliable, and makes it very easy to set up the \"boot disk\" for the Raspberry Pi. Running the bootstrap The bootstrap is a bash script that does three things: The bootstrap runs input-initial-configuration . This is a go program 18F developed to: Read in the api.data.gov access token. Read in the FCFS Seq Id for the library where the Pi is deployed. Read in a hardware identification tag that helps the librarian know where the device is installed (eg. \"reference desk\"). The bootstrap makes sure git is installed. It installs the most recent version of ansible from the official Ansible PPA. It pulls the imls-client-pi-playbook repository and runs the ansible playbook contained therein. The librarian runs the bootstrap by opening a terminal and pasting in a command that looks like bash <(curl -s ...) , where the URL is to a file in a Github repository. This downloads the script and executes it. Not only is the bootstrap never used again, it is not possible to use the bootstrap a second time, as we will explain while discussing the playbook . About the bootstrap We considered multiple paths to bootstrapping a Raspberry Pi. There are custom IoT operating systems that can be used or licensed. For a pilot, this was not a path we wanted to explore. These often have specific packaging and signing requirements, and further, we were not confident that a librarian (working unsupported) could actually configure a device running one of these operating systems. We considered building a custom Raspbian image that \"baked in\" the setup scripts. However, there was always the problem of getting an api.data.gov key onto the device. Either a librarian had to boot the Pi and enter the key, or some kind of tricky \"name your keyfile this and save it on a USB stick called this \" would have been necessary. This is what led us to our final approach, which was... Use the stock operating system image. This is the image that has the highest probability of being up-to-date, most secure, and most easily used as a \"fixed point\" in our configuration. It has the best support for installation (the imager \"just works\"), and the bootstrap script becomes a single-line copy-paste instruction to the librarian. If we were scaling, we could consider other approaches, but the questions would be come: Is another approach easier for the user? Is another approach less error prone for the user? Is another approach more secure for the user? and each of these questions (and others) would have to be held in balance with each-other. We believe our bootstrap solution is simple, reasonably secure, and able to be carried out by a wide range of users.","title":"Bootstrap"},{"location":"setup/bootstrap/#bootstrap","text":"To set up a Raspberry Pi, a librarian first needs to install the Raspbian OS onto a microSD card, and then they need to run our bootstrap script.","title":"Bootstrap"},{"location":"setup/bootstrap/#installing-the-os","text":"x To setup the Raspberry Pi, we first ask the user to download the Raspberry Pi Imager . This is a free and open tool for Mac, Windows, and Linux that aids in the creation of microSD cards for the Pi. The imaging tool allows the user to select their operating system (there are multiple options), and then install it on a microSD card plugged into their computer. It is free, open, reliable, and makes it very easy to set up the \"boot disk\" for the Raspberry Pi.","title":"Installing the OS"},{"location":"setup/bootstrap/#running-the-bootstrap","text":"The bootstrap is a bash script that does three things: The bootstrap runs input-initial-configuration . This is a go program 18F developed to: Read in the api.data.gov access token. Read in the FCFS Seq Id for the library where the Pi is deployed. Read in a hardware identification tag that helps the librarian know where the device is installed (eg. \"reference desk\"). The bootstrap makes sure git is installed. It installs the most recent version of ansible from the official Ansible PPA. It pulls the imls-client-pi-playbook repository and runs the ansible playbook contained therein. The librarian runs the bootstrap by opening a terminal and pasting in a command that looks like bash <(curl -s ...) , where the URL is to a file in a Github repository. This downloads the script and executes it. Not only is the bootstrap never used again, it is not possible to use the bootstrap a second time, as we will explain while discussing the playbook .","title":"Running the bootstrap"},{"location":"setup/bootstrap/#about-the-bootstrap","text":"We considered multiple paths to bootstrapping a Raspberry Pi. There are custom IoT operating systems that can be used or licensed. For a pilot, this was not a path we wanted to explore. These often have specific packaging and signing requirements, and further, we were not confident that a librarian (working unsupported) could actually configure a device running one of these operating systems. We considered building a custom Raspbian image that \"baked in\" the setup scripts. However, there was always the problem of getting an api.data.gov key onto the device. Either a librarian had to boot the Pi and enter the key, or some kind of tricky \"name your keyfile this and save it on a USB stick called this \" would have been necessary. This is what led us to our final approach, which was... Use the stock operating system image. This is the image that has the highest probability of being up-to-date, most secure, and most easily used as a \"fixed point\" in our configuration. It has the best support for installation (the imager \"just works\"), and the bootstrap script becomes a single-line copy-paste instruction to the librarian. If we were scaling, we could consider other approaches, but the questions would be come: Is another approach easier for the user? Is another approach less error prone for the user? Is another approach more secure for the user? and each of these questions (and others) would have to be held in balance with each-other. We believe our bootstrap solution is simple, reasonably secure, and able to be carried out by a wide range of users.","title":"About the bootstrap"},{"location":"setup/playbook/","text":"Ansible Playbook We attempted to bootstrap the device using as little bash as possible. While ubiquitous as a shell/programming language, it is fragile and error prone. Therefore, we do as little in bash as possible, and immediately bootstrap to ansible , a widely used open source server automation framework supported by Red Hat. Roles Ansible arranges automation into \"playbooks,\" which are YAML documents describing actions to be taken on a server. We have one playbook arranged into multiple \"roles.\" Each role is responsible for a different aspect of the Raspberry Pi configuration. input-initial-configuration : Installs and runs our tool for gathering the FCFS Seq Id and API token from the librarian. packages : We begin with a role that takes responsibility for installing, updating, and removing packages from the Pi. For example, we install lshw , a tool for reporting on the hardware connected to the Pi, but remove packages like apache and nginx , because we do not want any externally visible services running on the device. unattended-upgrades : This role makes sure that unattended upgrades are enabled on the Raspberry Pi. This way, the device automatically gets critical service updates from the Debian package repositories. session-counter : Configures and installs the software for monitoring wifi usage. configure-monitor-mode : Configures the Raspberry Pi hardware for sensing and data collection. lockdown : This package makes a few changes. One, it brings up a firewall that prohibits all external connections to the Raspberry Pi. Two, it disables login for all accounts. At this point, it becomes impossible to log into the Pi ever again. In short, configuring a Raspberry Pi for use with our tools is a one-way trip, and not even the librarian can access the device after configuration. devsec.hardening.os_hardening : Uses an externally provided playbook that is intended to be compliant with the Inspec DevSec Baselines. This particular playbook is written against the Linux baseline. devsec.hardening.ssh_hardening : Uses an externally provided playbook that is intended to be compliant with the Inspec DevSec Baselines. This particular playbook is written against the SSH baseline. Between 6, 7, and 8, the Raspberry Pi is now: Inaccessible : there are no open ports, and even if the device is plugged in, no accounts can log in interactively. Hardened : we have run playbooks that intend to be compliant against baselines in order to limit access to the Pi. Maintenance When the playbook is done, the Pi is ready for use. This same playbook is then run periodically (daily), and in this way, we can later update the devices in production. We might add packages, change configuration, and so on... but we can do so knowing that no one has modified the device without our knowledge. To modify the playbook, someone would need commit access to a Github repository. Therefore, managing who has access to the playbook is critical to managing the security of the Raspberry Pis in production. We think this is reasonable. Caveats We believe we have brought a healthy level of paranoia to our development process. As configured by the playbook, are very close to completely meeting the Iot Device Cybersecurity Capability Core Baseline (NISTIR 8259A). However, we want to highlight two caveats: Ownership . If this were to scale, it is important to remember that the Pis being used are not owned or controlled by the federal government. They will be devices purchased and set up by libraries, and whether or not NIST or other controls apply is potentially a subject for debate. Our intent is that they use the tools we've developed, as a result we believe that the resulting \"hardened\" Pi is no longer an easily hacked device that might become part of an attack surface/vector within a library. Theft . If someone steals a Pi from the library, all bets are off . Someone could remove the microSD card from the Pi and read it/modify it/etc. At that point, our access controls do not matter. Further, because of how the Pi is designed, we cannot encrypt the filesystem of the Raspberry Pi . This is just one of many reasons that we do not store any data on the local filesystem. There are ways we could encrypt the Pis. Most of those solutions will require a librarian to physically interact with the device in case of a power outage, the device being unplugged, and so on. At that point, the utility of the Raspberry Pi as an \"automatic\" and \"autonomous\" sensor is greatly reduced. We believe we have made appropriate trade-offs in terms of security and utility in our design, and are not putting libraries or our communities at risk through our design.","title":"Playbook"},{"location":"setup/playbook/#ansible-playbook","text":"We attempted to bootstrap the device using as little bash as possible. While ubiquitous as a shell/programming language, it is fragile and error prone. Therefore, we do as little in bash as possible, and immediately bootstrap to ansible , a widely used open source server automation framework supported by Red Hat.","title":"Ansible Playbook"},{"location":"setup/playbook/#roles","text":"Ansible arranges automation into \"playbooks,\" which are YAML documents describing actions to be taken on a server. We have one playbook arranged into multiple \"roles.\" Each role is responsible for a different aspect of the Raspberry Pi configuration. input-initial-configuration : Installs and runs our tool for gathering the FCFS Seq Id and API token from the librarian. packages : We begin with a role that takes responsibility for installing, updating, and removing packages from the Pi. For example, we install lshw , a tool for reporting on the hardware connected to the Pi, but remove packages like apache and nginx , because we do not want any externally visible services running on the device. unattended-upgrades : This role makes sure that unattended upgrades are enabled on the Raspberry Pi. This way, the device automatically gets critical service updates from the Debian package repositories. session-counter : Configures and installs the software for monitoring wifi usage. configure-monitor-mode : Configures the Raspberry Pi hardware for sensing and data collection. lockdown : This package makes a few changes. One, it brings up a firewall that prohibits all external connections to the Raspberry Pi. Two, it disables login for all accounts. At this point, it becomes impossible to log into the Pi ever again. In short, configuring a Raspberry Pi for use with our tools is a one-way trip, and not even the librarian can access the device after configuration. devsec.hardening.os_hardening : Uses an externally provided playbook that is intended to be compliant with the Inspec DevSec Baselines. This particular playbook is written against the Linux baseline. devsec.hardening.ssh_hardening : Uses an externally provided playbook that is intended to be compliant with the Inspec DevSec Baselines. This particular playbook is written against the SSH baseline. Between 6, 7, and 8, the Raspberry Pi is now: Inaccessible : there are no open ports, and even if the device is plugged in, no accounts can log in interactively. Hardened : we have run playbooks that intend to be compliant against baselines in order to limit access to the Pi.","title":"Roles"},{"location":"setup/playbook/#maintenance","text":"When the playbook is done, the Pi is ready for use. This same playbook is then run periodically (daily), and in this way, we can later update the devices in production. We might add packages, change configuration, and so on... but we can do so knowing that no one has modified the device without our knowledge. To modify the playbook, someone would need commit access to a Github repository. Therefore, managing who has access to the playbook is critical to managing the security of the Raspberry Pis in production. We think this is reasonable.","title":"Maintenance"},{"location":"setup/playbook/#caveats","text":"We believe we have brought a healthy level of paranoia to our development process. As configured by the playbook, are very close to completely meeting the Iot Device Cybersecurity Capability Core Baseline (NISTIR 8259A). However, we want to highlight two caveats: Ownership . If this were to scale, it is important to remember that the Pis being used are not owned or controlled by the federal government. They will be devices purchased and set up by libraries, and whether or not NIST or other controls apply is potentially a subject for debate. Our intent is that they use the tools we've developed, as a result we believe that the resulting \"hardened\" Pi is no longer an easily hacked device that might become part of an attack surface/vector within a library. Theft . If someone steals a Pi from the library, all bets are off . Someone could remove the microSD card from the Pi and read it/modify it/etc. At that point, our access controls do not matter. Further, because of how the Pi is designed, we cannot encrypt the filesystem of the Raspberry Pi . This is just one of many reasons that we do not store any data on the local filesystem. There are ways we could encrypt the Pis. Most of those solutions will require a librarian to physically interact with the device in case of a power outage, the device being unplugged, and so on. At that point, the utility of the Raspberry Pi as an \"automatic\" and \"autonomous\" sensor is greatly reduced. We believe we have made appropriate trade-offs in terms of security and utility in our design, and are not putting libraries or our communities at risk through our design.","title":"Caveats"},{"location":"software/find-ralink/","text":"find-ralink find-ralink is used from within the setup playbook. It must be run sudo , as only root has privs to read the hardware. ( find-ralink relies on lshw , which it runs and then parses output from.) Typical usage looks like: find-ralink by itself will report back the logical device where the USB adapter is mounted; e.g. wlan1 . find-ralink --search ral --field all --extract logicaldevice will search all hardware descriptor fields of all attached network devices for the string ral , and if found, return the logical device name. find-ralink --discover --extract mac will attempt to discover a valid device, and after found, return the MAC address of that device. These flags are used in the playbook for detecting the presence of an RAlink-based USB wifi adapter and extracting information about them. A file ( search.json ) is placed in /etc by the playbook; this file contains search parameters for finding possibly compatible USB wifi adapters. Currently, we know of two that can work, but in the future, more devices can be supported. If the file is not found, an embedded version of the same data serves as a fallback. This documentation may someday fall out of sync with the code. For long term maintenance, it may be that moving all documentation to the code bases themselves, and eliminating this site, is a good idea.","title":"Find Wifi Hardware"},{"location":"software/find-ralink/#find-ralink","text":"find-ralink is used from within the setup playbook. It must be run sudo , as only root has privs to read the hardware. ( find-ralink relies on lshw , which it runs and then parses output from.) Typical usage looks like: find-ralink by itself will report back the logical device where the USB adapter is mounted; e.g. wlan1 . find-ralink --search ral --field all --extract logicaldevice will search all hardware descriptor fields of all attached network devices for the string ral , and if found, return the logical device name. find-ralink --discover --extract mac will attempt to discover a valid device, and after found, return the MAC address of that device. These flags are used in the playbook for detecting the presence of an RAlink-based USB wifi adapter and extracting information about them. A file ( search.json ) is placed in /etc by the playbook; this file contains search parameters for finding possibly compatible USB wifi adapters. Currently, we know of two that can work, but in the future, more devices can be supported. If the file is not found, an embedded version of the same data serves as a fallback. This documentation may someday fall out of sync with the code. For long term maintenance, it may be that moving all documentation to the code bases themselves, and eliminating this site, is a good idea.","title":"find-ralink"},{"location":"software/input-initial-configuration/","text":"input-initial-configuration This application provides a friendly interface to inputting three critical pieces of information. It is run from the bootstrap, before the playbook is executed. input-initial-configuration --fcfs-seq --tag --word-pairs --write is the typical use. This: Prompts for the FCFS Seq Id, and checks it (against a pattern). Reads in a hardware tag (e.g. \"device 001\" or \"networking closet\"). Reads in word pairs representing an api.data.gov key, checking the validity of each wordpair, and converting it to its appropriate three-character counterpart. Writes the data to auth.yaml in /etc/session-counter, encrypting and base64 encoding the API key along the way. This application embeds a file containing 500K word pairs. The ordering of the pairs matters. We look up a wordpair, get its index, and then use that as an 18-bit binary value representing three ASCII characters. Because api.data.gov keys only use a limited character set, we can use 6 (as opposed to 8) bits per character to represent possible values in the key, and therefore use an 18-bit value to represent a 3-character \"chunk\" of the 40-character API key. Hence, we can encode a 40-character API key as 14 \"word pairs,\" which are easier for a librarian to read/type and easier for us to verify the correctness of. If that didn't make sense, the source code is your best bet. There is a corresponding encoder embedded in the setup documentation. That javascript implementation embeds the same 500K wordpair list; if those lists fall out of sync, the encoding will not work .","title":"Initial Configuration"},{"location":"software/input-initial-configuration/#input-initial-configuration","text":"This application provides a friendly interface to inputting three critical pieces of information. It is run from the bootstrap, before the playbook is executed. input-initial-configuration --fcfs-seq --tag --word-pairs --write is the typical use. This: Prompts for the FCFS Seq Id, and checks it (against a pattern). Reads in a hardware tag (e.g. \"device 001\" or \"networking closet\"). Reads in word pairs representing an api.data.gov key, checking the validity of each wordpair, and converting it to its appropriate three-character counterpart. Writes the data to auth.yaml in /etc/session-counter, encrypting and base64 encoding the API key along the way. This application embeds a file containing 500K word pairs. The ordering of the pairs matters. We look up a wordpair, get its index, and then use that as an 18-bit binary value representing three ASCII characters. Because api.data.gov keys only use a limited character set, we can use 6 (as opposed to 8) bits per character to represent possible values in the key, and therefore use an 18-bit value to represent a 3-character \"chunk\" of the 40-character API key. Hence, we can encode a 40-character API key as 14 \"word pairs,\" which are easier for a librarian to read/type and easier for us to verify the correctness of. If that didn't make sense, the source code is your best bet. There is a corresponding encoder embedded in the setup documentation. That javascript implementation embeds the same 500K wordpair list; if those lists fall out of sync, the encoding will not work .","title":"input-initial-configuration"},{"location":"software/session-counter/","text":"Session counter is run and managed from a systemd unit. It comes up after boot, and will be auto-restarted in the case of multiple HTTP failures. Below is an example of running session-counter interactively.","title":"Session Counter"},{"location":"stack/apidatagov/","text":"Overview api.data.gov provides free API management services for federal agencies. (The underlying framework is API Umbrella ). We use this service primarily for managing users and API keys. Using an API layer in this way allows us to provide transparent and seamless (to the user) backend updates. As a secondary benefit, we also get rate limiting and usage statistics. Please note that all API calls to api.data.gov must have a X-Api-Key request header with a valid API key for the path in question. Otherwise, the user will get an API_KEY_INVALID response. Configuration We have set up a predefined path at /TEST/10x-imls/ . To route the API call properly, we want to specify the Rabbit and Directus hosts to use. Since everything goes through Rabbit first (for validation purposes), we have configured Rabbit to read Directus configuration data via request headers. Our application sends a request to api.data.gov/TEST/10x-imls/v1/ with the appropriate X-Api-Key header and key api.data.gov looks up /TEST/10x-imls/ , and routes the request to our configuration api.data.gov looks up /v1/ in our API backend list The backend host is identified as 10x-rabbit-demo.app.cloud.gov Our \"Global Request Settings\" configuration tells api.data.gov to add these Rabbit-specific headers: X-Magic-Header X-Directus-Host X-Directus-Token X-Directus-Schema-Version Finally, the modified request is proxied (passed on) to the backend host with the path /validate/ Thus, a request to https://api.data.gov/TEST/10x-imls/v1/ is routed to https://10x-rabbit-demo.app.cloud.gov/validate/ with additional headers. Updates Currently we only offer the /v1/ path, but further revisions to our server stack are quite likely. Should we add /v2/ in the future, we will push an ansible update that configures our application to hit /v2/ instead. Because the backend beyond api.data.gov is essentially invisible to the user, the new endpoint could use entirely new backend services if needed.","title":"api.data.gov"},{"location":"stack/apidatagov/#overview","text":"api.data.gov provides free API management services for federal agencies. (The underlying framework is API Umbrella ). We use this service primarily for managing users and API keys. Using an API layer in this way allows us to provide transparent and seamless (to the user) backend updates. As a secondary benefit, we also get rate limiting and usage statistics. Please note that all API calls to api.data.gov must have a X-Api-Key request header with a valid API key for the path in question. Otherwise, the user will get an API_KEY_INVALID response.","title":"Overview"},{"location":"stack/apidatagov/#configuration","text":"We have set up a predefined path at /TEST/10x-imls/ . To route the API call properly, we want to specify the Rabbit and Directus hosts to use. Since everything goes through Rabbit first (for validation purposes), we have configured Rabbit to read Directus configuration data via request headers. Our application sends a request to api.data.gov/TEST/10x-imls/v1/ with the appropriate X-Api-Key header and key api.data.gov looks up /TEST/10x-imls/ , and routes the request to our configuration api.data.gov looks up /v1/ in our API backend list The backend host is identified as 10x-rabbit-demo.app.cloud.gov Our \"Global Request Settings\" configuration tells api.data.gov to add these Rabbit-specific headers: X-Magic-Header X-Directus-Host X-Directus-Token X-Directus-Schema-Version Finally, the modified request is proxied (passed on) to the backend host with the path /validate/ Thus, a request to https://api.data.gov/TEST/10x-imls/v1/ is routed to https://10x-rabbit-demo.app.cloud.gov/validate/ with additional headers.","title":"Configuration"},{"location":"stack/apidatagov/#updates","text":"Currently we only offer the /v1/ path, but further revisions to our server stack are quite likely. Should we add /v2/ in the future, we will push an ansible update that configures our application to hit /v2/ instead. Because the backend beyond api.data.gov is essentially invisible to the user, the new endpoint could use entirely new backend services if needed.","title":"Updates"},{"location":"stack/directus/","text":"","title":"Directus"},{"location":"stack/reval/","text":"ReVal ReVal (Reusable Validation Library) is a Django application for validating data via an API and web interface. ReVal was originally developed for the USDA FNS (Food and Nutrition Service) Data Validation Service in order to validate National School Lunch and Breakfast data . We have configured our ReVal instance, called Rabbit , to be a stateless validation application deployed on cloud.gov at 10x-rabbit-demo.app.cloud.gov . Configuration Rabbit provides one endpoint: /validate/<collection>/ . The only action allowed is POST . This endpoint takes an arbitrary array of JSON data, grabs the corresponding validation schema for that collection from the Directus host given, validates data against the schema, and returns the result of validation, successful or otherwise. Rabbit requires three HTTP headers: X-Magic-Header : secret key for the rabbit instance X-Directus-Host : Directus host (currently 10x-rabbit-data.app.cloud.gov ) X-Directus-Token : Directus token Errors from the Directus instance (if any) will be returned verbatim. Otherwise, the endpoint returns a standard ReVal validation object in JSON . Usage We proxy all api.data.gov requests through Rabbit for validation purposes: the data must be a JSON array of predefined objects. We use the following validation schemas: events and wifi . All Rabbit requests are also logged to a separate, generic Directus table. Should the data not pass validation, the resulting validation errors are also stored in a separate table. The current v1 schema is defined here . To avoid abuse of any Rabbit endpoint, we mandate a secret key to be passed in via the X-Magic-Header . This header is set on the api.data.gov backend configuration.","title":"ReVal (Rabbit)"},{"location":"stack/reval/#reval","text":"ReVal (Reusable Validation Library) is a Django application for validating data via an API and web interface. ReVal was originally developed for the USDA FNS (Food and Nutrition Service) Data Validation Service in order to validate National School Lunch and Breakfast data . We have configured our ReVal instance, called Rabbit , to be a stateless validation application deployed on cloud.gov at 10x-rabbit-demo.app.cloud.gov .","title":"ReVal"},{"location":"stack/reval/#configuration","text":"Rabbit provides one endpoint: /validate/<collection>/ . The only action allowed is POST . This endpoint takes an arbitrary array of JSON data, grabs the corresponding validation schema for that collection from the Directus host given, validates data against the schema, and returns the result of validation, successful or otherwise. Rabbit requires three HTTP headers: X-Magic-Header : secret key for the rabbit instance X-Directus-Host : Directus host (currently 10x-rabbit-data.app.cloud.gov ) X-Directus-Token : Directus token Errors from the Directus instance (if any) will be returned verbatim. Otherwise, the endpoint returns a standard ReVal validation object in JSON .","title":"Configuration"},{"location":"stack/reval/#usage","text":"We proxy all api.data.gov requests through Rabbit for validation purposes: the data must be a JSON array of predefined objects. We use the following validation schemas: events and wifi . All Rabbit requests are also logged to a separate, generic Directus table. Should the data not pass validation, the resulting validation errors are also stored in a separate table. The current v1 schema is defined here . To avoid abuse of any Rabbit endpoint, we mandate a secret key to be passed in via the X-Magic-Header . This header is set on the api.data.gov backend configuration.","title":"Usage"}]}